{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.4153, -0.9413,  0.8550, -1.3293,  1.6079],\n        [ 0.7442, -0.7293,  0.9848,  0.7392, -1.3974],\n        [ 0.8280,  0.4740, -0.0798,  0.3997, -1.1896],\n        [-1.2262, -1.8494,  1.4618,  0.5002,  0.3382],\n        [-0.6768,  1.2837, -1.1490,  0.0199,  1.5975],\n        [ 1.2886, -0.4951,  1.5465,  0.5698,  0.6618],\n        [ 0.4021, -0.4294,  0.1554, -0.7145,  0.9706],\n        [ 2.2023, -0.8830,  0.5127, -0.3469, -0.4431],\n        [-1.5755,  3.0294,  0.2419,  1.5847, -0.2715],\n        [-0.8835, -1.1849, -0.8417, -0.9277, -0.8115]], requires_grad=True)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Embeding\n",
    "embed = nn.Embedding(10, 5)  # 10个词 5维度 —— 词表\n",
    "input = torch.LongTensor([1, 5, 8])  # 三个词在词表中的位置 输入词索引\n",
    "embed_vector = embed(input)  # 将词索引映射为词向量 —— 词输入作用到词表 这句话每个词的词向量\n",
    "\n",
    "embed.weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.7442, -0.7293,  0.9848,  0.7392, -1.3974],\n        [ 1.2886, -0.4951,  1.5465,  0.5698,  0.6618],\n        [-1.5755,  3.0294,  0.2419,  1.5847, -0.2715]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vector  # 取出"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.4328, -0.3348,  0.1672, -0.2531,  0.0499],\n        [-0.1833,  0.1492, -0.3691,  0.0408, -0.4445],\n        [-0.2660, -0.0177, -0.1482, -0.1242,  0.1807],\n        [ 0.1188,  0.0344, -0.3487, -0.4196,  0.2209],\n        [-0.3169, -0.3924,  0.3597,  0.2004,  0.4252],\n        [ 0.3007, -0.3972, -0.0343,  0.2566,  0.3572],\n        [-0.1156, -0.3492, -0.2855, -0.2628, -0.0105],\n        [ 0.0995, -0.2521,  0.3069,  0.0201, -0.0081],\n        [ 0.0108, -0.3763,  0.2136,  0.2867, -0.2415],\n        [-0.3875, -0.1295,  0.2180, -0.0164,  0.1174]], requires_grad=True)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Linear\n",
    "fc = nn.Linear(5, 10)  # 输入维度5 输出维度10\n",
    "input = torch.randn(3, 5)  # 3个5维的输入\n",
    "output = fc(input)  # 全连接层映射到10维输出\n",
    "\n",
    "fc.weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0029,  1.1986, -0.9221,  0.5691, -2.6573, -0.6776,  0.2972, -0.1933,\n         -0.0282, -1.2522],\n        [-0.9954, -0.4001,  0.5952,  0.4435,  1.1623,  0.5016,  0.7252, -0.0366,\n         -0.1845,  0.8951],\n        [ 1.1373, -0.7050, -0.2704,  1.3825, -0.7205,  0.1895,  0.9271,  0.4831,\n         -0.3922, -0.2671]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output  # 点乘"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 简单实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.7559,  0.8514, -0.8851, -0.2470, -1.1481],\n        [-0.5923,  0.1025, -0.0075,  0.0431,  2.0494]], requires_grad=True)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix = {'hello': 0, 'world': 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2个词 5维度 —— 词表\n",
    "lookup_tensor = torch.tensor([word_to_ix['hello']], dtype=torch.long)  # 输入的词索引\n",
    "hello_embed = embeds(lookup_tensor)  # 得到词向量\n",
    "\n",
    "embeds.weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.7559,  0.8514, -0.8851, -0.2470, -1.1481]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_embed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## transformer中的实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\" d_model为词嵌入维度、vocab为词典大小\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)  # self.lut为词表\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" 前向传播 \"\"\"\n",
    "        return self.lut(x) * math.sqrt(self.d_model)  # 得到词向量  归一化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 27.5474,   0.2073,   3.2926,  ...,   7.0026,  12.8985,  13.7053],\n         [ 15.3772,  29.1237,  39.7162,  ...,  13.3172, -12.9345,  -8.1200],\n         [ 39.2212, -23.5262,  -3.6106,  ...,  28.7280, -24.4168,  18.7876],\n         [  1.6401,  21.7330, -11.0413,  ...,  27.2853, -24.3785, -14.1046]],\n\n        [[ 41.1038,  -5.4731,  24.1499,  ...,  26.1438,   7.1153,   4.9907],\n         [-14.9304, -22.4000,  32.7507,  ..., -25.9331, -23.8217, -15.4093],\n         [-36.3945,  -4.3041, -28.6258,  ...,   8.7273, -28.3143,  -4.1974],\n         [-13.3717,  -0.7670,  10.8779,  ...,  13.5576, -50.2387, -38.9500]]],\n       grad_fn=<MulBackward0>)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model, vocab = 512, 1000  #  1000个词 512维度\n",
    "x = torch.tensor([[100, 2, 421, 508], [491, 998, 1, 221]], dtype=torch.long)  # 输入预料的词索引\n",
    "emb = Embeddings(d_model, vocab)\n",
    "embr = emb(x)\n",
    "\n",
    "embr  # torch.Size([2, 4, 512])  2句话 每句4个词 每词512维度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "{'away': 0,\n 'useful': 1,\n 'an': 2,\n 'cinema': 3,\n 'old': 4,\n 'a': 5,\n 'tool': 6,\n 'far': 7,\n 'english': 8,\n 'worker': 9,\n 'is': 10,\n 'he': 11,\n 'the': 12}"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#已知如下语料 给出其词嵌入\n",
    "corpus = [\"he is an old worker\", \"english is a useful tool\", \"the cinema is far away\"]\n",
    "\n",
    "# 需要生成词表\n",
    "word_list = []\n",
    "for sentence in corpus:  # sentence为1句话\n",
    "    for word in sentence.split():  # word为1个词\n",
    "        word_list.append(word)\n",
    "# 需要去重 且生成词构成的索引结构\n",
    "word_dirt = {}\n",
    "for i in enumerate(set(word_list)):\n",
    "    word_dirt[i[1]] = i[0]\n",
    "\n",
    "word_dirt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.1652,  1.8527, -0.5448, -0.2729, -0.1449],\n        [ 1.4996, -1.0888,  0.9264, -0.7588,  0.6251],\n        [-1.1544, -0.2262, -0.7112,  0.1644,  0.3335],\n        [ 0.0692, -0.6363,  0.0714, -2.1863, -0.9576],\n        [-0.4400, -0.4123, -0.1642, -0.1696, -1.1739],\n        [ 0.5537, -1.3774, -0.1361, -1.0780,  0.4689],\n        [-1.7460,  0.8354,  0.7688, -1.4681, -0.5667],\n        [-0.0781,  0.1157,  0.8574,  0.2917, -0.1305],\n        [-0.4630,  0.3838,  0.4448, -0.4246, -2.2026],\n        [ 0.9852, -0.2545,  0.5236, -1.9448, -0.4065],\n        [ 0.3120,  0.7543, -0.1107,  0.2004, -0.5817],\n        [-1.6479,  0.3058, -1.3786,  0.5189,  1.1430],\n        [-0.6112,  0.0971, -0.3067, -0.5186,  1.2675]], requires_grad=True)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词嵌入矩阵\n",
    "embeds = nn.Embedding(len(word_dirt), 5)  # 13个词 5维度\n",
    "# 输入一句话  tensor([11, 10,  2,  4,  9])\n",
    "lookup_tensor = torch.tensor([word_dirt[word] for word in corpus[0].split()], dtype=torch.long)\n",
    "# 得到这句话每个词的词向量\n",
    "sub_embed = embeds(lookup_tensor)\n",
    "\n",
    "embeds.weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.6479,  0.3058, -1.3786,  0.5189,  1.1430],\n        [ 0.3120,  0.7543, -0.1107,  0.2004, -0.5817],\n        [-1.1544, -0.2262, -0.7112,  0.1644,  0.3335],\n        [-0.4400, -0.4123, -0.1642, -0.1696, -1.1739],\n        [ 0.9852, -0.2545,  0.5236, -1.9448, -0.4065]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_embed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 把现有预料处理成训练集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "[(['forty', 'When'], 'winters'),\n (['winters', 'forty'], 'shall'),\n (['shall', 'winters'], 'besiege'),\n (['besiege', 'shall'], 'thy'),\n (['thy', 'besiege'], 'brow,'),\n (['brow,', 'thy'], 'And'),\n (['And', 'brow,'], 'dig'),\n (['dig', 'And'], 'deep'),\n (['deep', 'dig'], 'trenches'),\n (['trenches', 'deep'], 'in'),\n (['in', 'trenches'], 'thy'),\n (['thy', 'in'], \"beauty's\"),\n ([\"beauty's\", 'thy'], 'field,'),\n (['field,', \"beauty's\"], 'Thy'),\n (['Thy', 'field,'], \"youth's\"),\n ([\"youth's\", 'Thy'], 'proud'),\n (['proud', \"youth's\"], 'livery'),\n (['livery', 'proud'], 'so'),\n (['so', 'livery'], 'gazed'),\n (['gazed', 'so'], 'on'),\n (['on', 'gazed'], 'now,'),\n (['now,', 'on'], 'Will'),\n (['Will', 'now,'], 'be'),\n (['be', 'Will'], 'a'),\n (['a', 'be'], \"totter'd\"),\n ([\"totter'd\", 'a'], 'weed'),\n (['weed', \"totter'd\"], 'of'),\n (['of', 'weed'], 'small'),\n (['small', 'of'], 'worth'),\n (['worth', 'small'], 'held:'),\n (['held:', 'worth'], 'Then'),\n (['Then', 'held:'], 'being'),\n (['being', 'Then'], 'asked,'),\n (['asked,', 'being'], 'where'),\n (['where', 'asked,'], 'all'),\n (['all', 'where'], 'thy'),\n (['thy', 'all'], 'beauty'),\n (['beauty', 'thy'], 'lies,'),\n (['lies,', 'beauty'], 'Where'),\n (['Where', 'lies,'], 'all'),\n (['all', 'Where'], 'the'),\n (['the', 'all'], 'treasure'),\n (['treasure', 'the'], 'of'),\n (['of', 'treasure'], 'thy'),\n (['thy', 'of'], 'lusty'),\n (['lusty', 'thy'], 'days;'),\n (['days;', 'lusty'], 'To'),\n (['To', 'days;'], 'say,'),\n (['say,', 'To'], 'within'),\n (['within', 'say,'], 'thine'),\n (['thine', 'within'], 'own'),\n (['own', 'thine'], 'deep'),\n (['deep', 'own'], 'sunken'),\n (['sunken', 'deep'], 'eyes,'),\n (['eyes,', 'sunken'], 'Were'),\n (['Were', 'eyes,'], 'an'),\n (['an', 'Were'], 'all-eating'),\n (['all-eating', 'an'], 'shame,'),\n (['shame,', 'all-eating'], 'and'),\n (['and', 'shame,'], 'thriftless'),\n (['thriftless', 'and'], 'praise.'),\n (['praise.', 'thriftless'], 'How'),\n (['How', 'praise.'], 'much'),\n (['much', 'How'], 'more'),\n (['more', 'much'], 'praise'),\n (['praise', 'more'], \"deserv'd\"),\n ([\"deserv'd\", 'praise'], 'thy'),\n (['thy', \"deserv'd\"], \"beauty's\"),\n ([\"beauty's\", 'thy'], 'use,'),\n (['use,', \"beauty's\"], 'If'),\n (['If', 'use,'], 'thou'),\n (['thou', 'If'], 'couldst'),\n (['couldst', 'thou'], 'answer'),\n (['answer', 'couldst'], \"'This\"),\n ([\"'This\", 'answer'], 'fair'),\n (['fair', \"'This\"], 'child'),\n (['child', 'fair'], 'of'),\n (['of', 'child'], 'mine'),\n (['mine', 'of'], 'Shall'),\n (['Shall', 'mine'], 'sum'),\n (['sum', 'Shall'], 'my'),\n (['my', 'sum'], 'count,'),\n (['count,', 'my'], 'and'),\n (['and', 'count,'], 'make'),\n (['make', 'and'], 'my'),\n (['my', 'make'], 'old'),\n (['old', 'my'], \"excuse,'\"),\n ([\"excuse,'\", 'old'], 'Proving'),\n (['Proving', \"excuse,'\"], 'his'),\n (['his', 'Proving'], 'beauty'),\n (['beauty', 'his'], 'by'),\n (['by', 'beauty'], 'succession'),\n (['succession', 'by'], 'thine!'),\n (['thine!', 'succession'], 'This'),\n (['This', 'thine!'], 'were'),\n (['were', 'This'], 'to'),\n (['to', 'were'], 'be'),\n (['be', 'to'], 'new'),\n (['new', 'be'], 'made'),\n (['made', 'new'], 'when'),\n (['when', 'made'], 'thou'),\n (['thou', 'when'], 'art'),\n (['art', 'thou'], 'old,'),\n (['old,', 'art'], 'And'),\n (['And', 'old,'], 'see'),\n (['see', 'And'], 'thy'),\n (['thy', 'see'], 'blood'),\n (['blood', 'thy'], 'warm'),\n (['warm', 'blood'], 'when'),\n (['when', 'warm'], 'thou'),\n (['thou', 'when'], \"feel'st\"),\n ([\"feel'st\", 'thou'], 'it'),\n (['it', \"feel'st\"], 'cold.')]"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "torch.manual_seed(1)\n",
    "\n",
    "test_sentence = \"\"\"\n",
    "When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "\n",
    "ngrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "{'And': 0,\n 'Will': 1,\n 'praise': 2,\n \"beauty's\": 3,\n 'warm': 4,\n \"'This\": 5,\n \"deserv'd\": 6,\n 'If': 7,\n 'be': 8,\n 'old,': 9,\n 'winters': 10,\n 'beauty': 11,\n 'dig': 12,\n 'blood': 13,\n 'of': 14,\n 'say,': 15,\n 'thy': 16,\n \"youth's\": 17,\n 'made': 18,\n 'own': 19,\n 'This': 20,\n 'when': 21,\n 'shall': 22,\n 'new': 23,\n 'small': 24,\n 'livery': 25,\n 'Thy': 26,\n 'couldst': 27,\n 'see': 28,\n 'on': 29,\n 'shame,': 30,\n 'eyes,': 31,\n \"feel'st\": 32,\n 'How': 33,\n 'held:': 34,\n 'Where': 35,\n 'much': 36,\n 'trenches': 37,\n 'it': 38,\n 'Then': 39,\n 'field,': 40,\n 'my': 41,\n 'to': 42,\n 'in': 43,\n 'the': 44,\n 'being': 45,\n 'thine!': 46,\n 'count,': 47,\n 'lies,': 48,\n 'by': 49,\n \"totter'd\": 50,\n 'make': 51,\n \"excuse,'\": 52,\n 'worth': 53,\n 'besiege': 54,\n 'sunken': 55,\n 'sum': 56,\n 'days;': 57,\n 'To': 58,\n 'child': 59,\n 'cold.': 60,\n 'succession': 61,\n 'old': 62,\n 'use,': 63,\n 'where': 64,\n 'praise.': 65,\n 'within': 66,\n 'a': 67,\n 'thine': 68,\n 'weed': 69,\n 'treasure': 70,\n 'asked,': 71,\n 'thriftless': 72,\n 'Proving': 73,\n 'all': 74,\n 'mine': 75,\n 'forty': 76,\n 'all-eating': 77,\n 'brow,': 78,\n 'were': 79,\n 'answer': 80,\n 'deep': 81,\n 'now,': 82,\n 'Were': 83,\n 'an': 84,\n 'art': 85,\n 'proud': 86,\n 'more': 87,\n 'When': 88,\n 'his': 89,\n 'so': 90,\n 'and': 91,\n 'fair': 92,\n 'lusty': 93,\n 'thou': 94,\n 'gazed': 95,\n 'Shall': 96}"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "word_to_ix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# 神经网络模型定义\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        \"\"\" 词表长度、词向量维度、上下文相关 \"\"\"\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0125, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7472, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4900, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2347, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9782, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7082, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4209, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1165, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7959, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4601, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1117, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7604, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4246, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1231, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8734, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6767, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5336, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4285, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3510, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2495, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2150, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1876, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1650, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1463, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1311, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1179, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1066, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0886, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0812, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0748, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0692, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0643, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0599, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0560, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0524, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0493, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0465, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0439, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0415, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0394, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0375, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0357, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0341, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0326, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0312, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0287, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0276, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0266, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0256, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0248, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0239, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0231, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0224, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0217, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0210, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0204, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0198, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0192, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0187, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0182, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0177, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0173, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0168, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0164, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0160, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0157, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0153, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0149, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0146, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0143, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0140, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0137, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0134, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0132, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0129, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0126, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0122, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0119, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0095, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0093, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0092, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0091, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0089, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):  # 100次训练\n",
    "    for i in ngrams:  # 1次训练\n",
    "        # 输入输出 不能用词要用词表的位置\n",
    "        input_x = torch.tensor([word_to_ix[i] for i in i[0]], dtype=torch.long)\n",
    "        input_y = torch.tensor([word_to_ix[i[1]]], dtype=torch.long)\n",
    "        output0 = model(input_x)\n",
    "\n",
    "        model.zero_grad()  # 之前的梯度数据清零\n",
    "        loss = loss_function(output0, input_y)  # 算损失\n",
    "        loss.backward()  # 损失 反向传播\n",
    "        optimer.step()  # 更新梯度 优化器\n",
    "    # 每次训练输出结果\n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 5.2145e-02,  7.7348e-01, -6.2138e-01,  1.5694e+00,  8.9320e-01,\n         -2.7682e-01,  8.2514e-01, -1.6480e+00,  9.3918e-01,  5.3340e-01],\n        [-1.7176e+00, -3.1630e-01,  8.9163e-01, -5.2749e-01,  4.9563e-01,\n          1.6146e+00, -2.3427e-01, -2.2426e-01,  1.1302e+00, -1.1990e+00],\n        [ 1.3525e+00, -2.3831e-01, -3.1669e-02, -7.8350e-02,  2.7794e-01,\n         -6.7688e-01,  1.6782e+00,  1.6762e+00,  1.0122e+00, -5.6545e-01],\n        [-1.3831e+00,  2.5403e-01,  7.0112e-01, -1.2497e+00,  2.7104e-01,\n         -8.3010e-01, -8.5585e-01, -1.2908e+00, -4.3944e-01,  1.1033e+00],\n        [-4.2272e-01, -4.6001e-01,  3.4660e+00, -6.5733e-01, -4.5240e-01,\n          1.3041e+00,  3.2442e-01, -2.8194e+00, -3.6490e-01, -1.1798e+00],\n        [-1.3210e+00, -2.0738e-01,  8.2298e-01,  6.8098e-01, -5.6163e-02,\n         -1.5267e+00,  8.5267e-01,  1.0257e+00, -1.3615e+00, -6.5622e-01],\n        [ 3.2922e-01,  9.9916e-01,  1.0737e+00, -1.0013e-01, -5.6900e-01,\n          2.9329e-01, -5.3216e-01, -1.2635e+00, -9.0646e-01,  6.1119e-01],\n        [ 1.2485e+00, -9.8210e-01, -9.4407e-02, -9.4745e-01, -8.6523e-01,\n         -4.7715e-02, -1.8663e+00, -7.7076e-02, -3.1090e-01, -3.3935e-01],\n        [ 1.6181e-01,  5.7689e-01,  1.8446e+00,  6.3074e-01, -4.0275e-01,\n          8.3154e-01,  8.5670e-02,  1.0976e+00, -6.3697e-01, -1.6060e+00],\n        [-2.1837e-01,  1.3895e+00,  4.7465e-03,  8.7949e-01, -1.3889e+00,\n         -4.2804e-01, -3.8778e-01, -7.3827e-01,  2.0764e+00, -8.3896e-02],\n        [ 1.4575e+00, -5.3755e-01, -9.3248e-01,  9.9885e-01,  1.3035e-01,\n         -7.3633e-01, -1.4309e+00, -1.9705e+00, -6.7586e-01,  7.0437e-01],\n        [ 1.2475e+00,  2.7108e-01, -7.6458e-01,  7.8948e-01, -1.1325e+00,\n         -4.1378e-02, -8.5883e-01, -5.3370e-02, -1.0862e+00, -1.1274e+00],\n        [-2.3419e+00, -4.2263e-01,  1.0004e+00, -9.4003e-01, -1.3964e+00,\n          1.2925e-01,  7.1184e-01,  1.3100e-01,  4.0859e-01,  9.9910e-01],\n        [-8.5491e-02,  1.0416e-01,  1.7562e+00,  8.6381e-01,  3.4499e-01,\n         -9.4703e-01,  9.0282e-01, -4.1102e-02,  2.7288e-01,  4.0696e-01],\n        [-1.6036e+00, -1.6153e+00,  7.5054e-01,  2.5768e+00,  1.4325e+00,\n          8.8607e-02, -7.3268e-01, -1.9950e-01,  2.9339e+00, -2.1145e+00],\n        [-9.0733e-01,  7.4279e-01, -6.4256e-01, -1.4326e+00,  1.2980e+00,\n         -2.4007e+00, -2.1684e-01,  3.2564e-01,  3.2252e-01, -1.3045e+00],\n        [ 5.5827e-01,  7.2653e-01,  5.8660e-01, -1.1008e+00,  2.9875e-01,\n          5.6706e-01, -1.3031e+00,  1.4131e+00, -1.9376e+00, -7.6165e-02],\n        [-1.0071e+00, -6.3308e-01, -1.0980e+00,  1.1187e+00, -6.4482e-01,\n          1.4951e+00, -1.4520e+00, -4.8399e-01, -9.6391e-01, -1.7559e-01],\n        [ 6.5524e-01, -2.4037e+00,  7.8210e-01,  4.7781e-01,  2.0875e-01,\n         -6.7496e-01,  5.6279e-01,  7.9696e-01,  3.8497e-01,  1.6429e+00],\n        [-5.9504e-02,  7.7533e-01, -5.4538e-01, -1.2382e+00, -3.5523e-01,\n         -6.0452e-01,  1.5494e-01, -2.7021e+00,  3.1679e-01, -5.3672e-01],\n        [-7.8665e-01, -6.6642e-02, -1.1953e+00,  5.7355e-01,  1.6492e+00,\n          7.5919e-01,  1.7479e+00,  3.8027e-01,  1.4943e+00,  2.3071e+00],\n        [-3.0632e-01,  1.9146e-02, -6.2934e-01, -9.4157e-01, -1.6596e+00,\n          1.1882e+00, -2.9293e-01,  3.7844e-01,  2.5800e+00,  2.2768e-01],\n        [ 5.0982e-01,  1.5078e-01,  2.3545e-01, -1.1438e+00,  9.8980e-01,\n         -5.1105e-01,  6.2914e-01,  1.7928e-01,  2.7531e-01, -7.7286e-01],\n        [ 3.2495e-01, -1.1907e+00, -9.9670e-01,  1.0099e+00, -7.1153e-01,\n         -1.0363e+00, -9.9986e-01,  1.0282e+00,  8.9900e-03,  1.1621e+00],\n        [-1.0463e-01,  7.3097e-01, -1.5913e+00,  4.2395e-01,  3.5963e-01,\n          2.2582e-01, -1.3067e+00, -4.7752e-01,  2.8578e-01, -7.9780e-01],\n        [ 1.1079e+00, -7.8149e-01, -1.2460e+00,  8.3336e-02,  1.5397e-01,\n         -5.7394e-01, -1.0897e+00,  1.8033e+00, -1.5586e+00, -3.7959e-01],\n        [ 8.8743e-01, -2.4882e+00, -3.5755e-01,  1.2532e+00,  8.7779e-01,\n          7.4522e-01, -7.1885e-01,  1.3007e-01, -1.1223e+00, -1.1541e+00],\n        [ 2.6722e-01,  2.2571e+00, -1.2260e+00,  5.2157e-01, -4.0939e-01,\n          9.0626e-01, -4.4447e-01, -6.5263e-01, -3.3625e-01,  6.5798e-02],\n        [-2.5712e-01, -1.4808e+00,  8.9936e-01, -1.1264e+00, -5.5167e-01,\n         -3.7950e-01, -4.6057e-01, -1.5139e-01,  4.6632e-02,  2.4907e+00],\n        [ 9.5600e-01,  1.4175e-01,  6.1205e-01, -1.7794e+00,  5.4007e-01,\n         -5.9645e-01,  4.3718e-01, -1.4483e+00, -5.5185e-02,  7.8538e-02],\n        [-1.4009e-01, -2.0119e-01, -8.5953e-01, -1.8012e+00, -1.0867e+00,\n         -7.4698e-01,  1.3978e-01,  1.1016e+00, -1.0572e+00, -2.5594e+00],\n        [-1.8607e+00, -1.0555e+00, -1.6066e+00,  6.3331e-01, -2.2090e+00,\n          1.5465e+00,  5.4565e-01,  4.4446e-01,  1.5768e-01,  2.8823e-01],\n        [-5.4135e-01, -4.9562e-01, -2.2045e+00,  2.4197e-01,  1.2112e+00,\n         -4.0781e-01, -1.6373e+00,  1.7471e+00,  4.8021e-01,  1.1471e+00],\n        [ 5.6849e-01,  4.8453e-01,  5.6749e-01,  2.5055e+00, -5.1183e-01,\n         -1.8056e+00,  2.0470e+00, -1.3072e+00, -1.2917e+00, -6.6456e-02],\n        [ 1.4422e+00,  1.0256e-02, -6.8420e-01,  6.0009e-01,  5.1766e-01,\n         -1.8458e-01, -1.6881e-01,  2.6881e-01,  3.4477e-01,  2.5318e-02],\n        [-8.6255e-01, -1.0268e-02, -6.7086e-02, -3.6921e-01, -1.4822e-01,\n          3.3997e-01,  6.5082e-01,  4.7631e-01,  1.1691e+00, -7.7645e-01],\n        [-1.6594e+00, -1.0154e+00, -9.0870e-01,  5.0640e-01, -1.0806e+00,\n         -8.2306e-01,  3.4675e-01,  1.3892e-01, -6.2161e-01,  8.8255e-01],\n        [ 9.5424e-01,  1.0587e+00,  6.9113e-02, -1.3131e+00,  6.5117e-01,\n          1.3893e+00,  2.3264e+00, -2.3293e+00,  5.9463e-01, -1.2639e+00],\n        [-1.3824e+00, -7.2057e-01, -9.0148e-01,  9.3449e-01,  2.2276e-01,\n         -3.5781e+00, -8.0916e-01, -3.9183e-02, -7.3962e-01, -1.4572e+00],\n        [-7.5646e-01, -1.4171e+00,  2.7221e-01,  6.2574e-01, -9.9386e-01,\n         -3.2430e-01,  8.6373e-01, -1.5646e+00, -1.1287e+00, -9.5232e-01],\n        [ 1.9190e-01,  1.3498e+00, -4.8464e-01,  7.4973e-01, -2.5010e-01,\n         -2.2893e+00,  3.3053e-01, -1.0755e+00,  9.8346e-02, -8.9213e-01],\n        [ 2.5967e-01, -2.4089e-01,  1.0172e+00,  1.2401e+00,  6.3794e-01,\n         -4.4052e-01,  6.3489e-01, -1.4250e+00,  1.2863e+00,  7.3026e-02],\n        [ 1.2786e+00, -6.9250e-01, -8.5368e-01, -2.4673e+00, -6.2146e-01,\n          1.5622e-01, -1.3946e+00, -5.9725e-01, -1.3842e+00, -1.5304e+00],\n        [-1.0365e+00,  6.2724e-02, -2.8156e-01,  1.3883e+00,  1.2924e+00,\n          1.4347e+00,  8.9847e-01, -7.6037e-01,  2.1145e-02,  2.3183e+00],\n        [-1.1212e+00,  1.4435e+00, -2.1971e-01,  6.9591e-01, -1.5301e+00,\n         -2.5055e-01, -2.4485e-01, -1.1077e+00,  1.4857e-01, -1.4414e+00],\n        [ 4.3204e-02, -1.0205e+00,  5.6739e-01, -4.9302e-01, -7.5295e-01,\n          1.8025e-01,  1.2333e+00, -2.0155e+00, -9.7125e-01, -2.5022e-02],\n        [-5.5024e-01, -3.9180e-01, -8.5863e-01, -1.6346e+00,  2.2308e-01,\n          1.1144e-01, -4.5644e-01, -1.2165e+00,  7.1383e-01,  1.0813e+00],\n        [-1.9054e+00, -4.2485e-01,  7.7236e-02,  1.7626e-01, -6.8922e-01,\n          1.7450e+00,  2.6033e-01, -5.0415e-01, -6.2805e-01,  2.2733e-02],\n        [-5.9057e-01, -2.0521e-01, -2.1333e+00,  7.8294e-01, -2.8523e-01,\n         -7.3810e-01,  1.4665e+00,  1.0896e+00,  4.7698e-01, -2.5539e-01],\n        [-1.0952e+00,  5.2426e-01, -2.4092e-01,  1.6737e+00, -1.5932e+00,\n         -1.4534e-01,  6.9719e-02,  4.8836e-01, -1.0017e+00,  1.2998e+00],\n        [-9.6657e-02,  2.6389e-01, -8.3147e-01, -2.3798e-01,  1.7866e+00,\n          1.4963e+00,  2.1071e+00, -1.2640e+00,  7.7225e-01, -1.4204e+00],\n        [ 2.3670e-01, -2.0764e-01, -1.4032e-01,  1.1512e-01,  1.1944e+00,\n          4.5755e-01, -2.0741e-01,  1.0406e+00, -8.9012e-01, -7.5885e-01],\n        [-1.5329e+00, -1.1439e+00,  1.0792e+00,  5.2867e-01, -1.5846e+00,\n         -2.6211e-01,  7.0511e-01,  1.1259e+00,  6.4192e-01, -1.4527e+00],\n        [ 1.7074e+00, -2.2136e+00,  2.0306e+00, -3.8643e-01,  6.8092e-01,\n          1.0623e+00, -1.5257e-01, -1.3590e+00, -1.1537e+00,  1.2534e+00],\n        [ 6.9097e-01, -5.3202e-01, -2.0319e+00, -5.0860e-01,  1.4076e+00,\n          1.2900e-01, -2.2843e+00,  5.8114e-01, -6.1561e-01, -2.5984e-01],\n        [-1.1507e+00, -1.4339e+00, -6.4644e-01, -1.8788e+00, -2.6885e-01,\n          7.4017e-01, -4.6762e-01, -2.6083e-01,  1.7191e+00,  2.3917e+00],\n        [ 1.6411e-01, -9.4899e-01, -6.5223e-01, -1.1532e+00, -9.2125e-01,\n         -8.3097e-01, -1.1863e+00,  4.8336e-01, -1.8746e+00, -1.8429e-01],\n        [-4.4243e-01, -1.6721e+00,  2.0516e-01,  7.0551e-01,  1.8647e+00,\n          3.9121e-01, -1.3789e+00,  6.5070e-01,  4.7996e-01, -7.7104e-01],\n        [-8.0657e-01,  2.5049e+00,  9.8483e-01, -2.6958e-01, -1.1951e+00,\n         -6.1815e-02,  6.9359e-01,  1.0667e+00,  8.1069e-01, -7.8569e-01],\n        [-4.1500e-01, -1.2341e+00,  2.3788e-01, -9.6864e-01, -1.0054e+00,\n         -7.3561e-01, -6.8971e-01, -1.9150e-01, -4.3693e-01,  1.5987e+00],\n        [-6.2691e-01,  1.9682e+00, -2.0883e-01, -2.7617e-01, -1.6535e-01,\n         -6.9492e-01,  2.0067e-01,  5.1614e-01,  2.9811e-01, -1.0386e-01],\n        [ 9.6512e-01,  1.2568e+00,  7.8520e-01,  8.4021e-01,  4.1863e-01,\n         -7.0389e-01,  1.0559e-01, -2.0861e+00, -3.3175e-02,  1.2845e+00],\n        [-1.6984e+00,  2.5959e-01, -8.9920e-01, -9.6511e-01, -1.1693e+00,\n          2.3223e+00, -4.3122e-01, -1.5669e+00, -1.0396e+00,  3.0307e+00],\n        [-8.7257e-01,  3.7268e-01,  1.0971e-01, -1.5188e-01, -5.6537e-01,\n         -7.7887e-01, -1.7965e+00,  3.9905e-01,  7.9936e-01, -1.5207e-02],\n        [-2.7907e-01, -3.0081e-01, -3.0155e-01, -8.8717e-01, -2.5824e-01,\n         -8.8839e-01, -9.5509e-01,  3.9905e-02, -1.4409e+00,  5.8788e-01],\n        [-1.7660e+00, -1.5094e-01, -3.2953e-01,  5.4954e-01, -1.0590e+00,\n          8.0656e-01,  1.1037e+00,  4.9538e-01, -9.5963e-01,  1.4675e+00],\n        [-1.1146e+00, -1.2066e+00,  4.4916e-01, -8.7265e-01,  1.1945e+00,\n         -3.3431e-01,  1.4934e+00, -1.0589e+00,  8.6895e-01,  2.4234e-02],\n        [ 4.8576e-01,  3.4745e-01,  8.8662e-01, -3.0910e-01, -4.1544e-01,\n         -2.3267e-01,  1.1077e+00,  9.5577e-01,  1.3633e-01,  6.9936e-01],\n        [ 8.0287e-01,  1.4866e+00, -1.0604e+00,  1.8448e+00, -2.3892e-01,\n         -1.9462e-01, -2.2545e+00, -1.4592e+00, -1.6771e-01,  1.5821e+00],\n        [ 1.9420e+00,  3.5386e-01,  1.3027e+00, -2.4186e-01, -1.2610e+00,\n         -3.2849e+00,  4.7551e-02, -3.8203e-01,  9.8924e-01, -5.1510e-01],\n        [-9.7962e-01, -7.5426e-01,  3.3595e-01,  1.1505e+00,  1.9015e+00,\n         -7.9429e-01, -1.7969e+00,  5.3564e-01, -3.6201e-01,  1.0414e+00],\n        [-2.3025e-02,  1.1401e+00, -9.8997e-01, -4.9227e-02, -1.6007e+00,\n         -1.6604e+00,  1.5328e+00,  1.8403e-01, -8.6409e-01,  1.6011e+00],\n        [-1.6129e-01, -7.4034e-01, -1.1940e-02, -4.3715e-01, -3.1276e-02,\n          1.9818e+00,  1.5945e+00, -1.7062e-01, -1.5136e-02, -6.9200e-01],\n        [-2.8664e-02,  2.2587e-01,  1.2653e-01,  9.6530e-01, -8.1035e-01,\n         -5.1253e-01, -1.5180e+00,  1.5893e+00,  1.1797e+00, -1.0945e+00],\n        [-9.2432e-01, -3.3940e-01, -3.3608e-01,  2.2320e+00,  2.2758e+00,\n         -1.8755e-01, -2.1068e-01, -2.9230e-01, -1.2247e+00, -8.3305e-01],\n        [ 2.5198e-01, -1.0672e+00, -2.8426e-01,  1.2380e+00,  1.2396e+00,\n         -2.9341e-01,  3.8725e-01,  1.2154e+00,  1.0539e+00, -1.4377e+00],\n        [ 1.1388e+00, -1.5370e+00, -1.4051e-01, -9.1841e-01, -2.1567e-01,\n          4.3390e-01,  8.3026e-01, -1.3829e+00,  1.4101e-01,  1.3379e-03],\n        [ 7.3173e-01,  8.9236e-01,  6.3980e-01, -4.4286e-01,  2.3023e+00,\n         -1.1173e-01, -1.2117e-01, -1.4382e+00,  1.3239e+00,  1.4137e+00],\n        [-1.4867e-01, -9.5999e-01, -1.2247e+00,  2.7774e+00, -1.9698e+00,\n          5.4692e-01, -6.1259e-01,  2.2956e-01, -3.0195e-01, -1.9784e+00],\n        [-1.3003e+00,  4.4951e-01,  1.7036e+00,  6.0568e-01, -6.0050e-01,\n         -1.4007e+00, -1.2886e+00, -8.5256e-01, -9.6388e-01,  1.1469e-01],\n        [-1.2511e+00, -1.4588e+00, -1.7951e+00,  1.0751e+00, -7.1957e-01,\n          1.0074e+00, -2.5257e-02, -4.0235e-01,  1.1045e+00,  2.6193e+00],\n        [ 2.4497e-01,  3.3368e-01, -9.2434e-01,  1.2825e+00, -8.7092e-01,\n          2.3506e+00, -7.3265e-01, -3.4269e-01, -8.1920e-01,  1.7726e-01],\n        [-1.5907e+00, -9.4633e-02,  5.6866e-01, -1.0095e+00,  3.9249e-01,\n          9.2689e-01, -3.4221e-01,  5.9945e-01, -1.1120e+00, -2.3361e+00],\n        [ 4.6040e-01,  4.0419e-01, -5.1867e-01,  3.3555e-01, -9.0658e-01,\n         -1.1400e+00, -6.4833e-01, -2.8504e+00,  1.2695e+00,  1.0200e+00],\n        [-1.1133e+00,  1.3166e+00, -2.5636e-01, -5.8668e-01, -5.0995e-01,\n          1.1455e+00, -6.5414e-02, -6.9228e-01, -7.9167e-01, -1.5812e+00],\n        [-2.8491e-01,  2.9079e-02,  1.3864e+00,  3.2665e-01, -4.9317e-02,\n         -5.9918e-01, -4.9215e-01,  1.6875e+00, -7.2602e-02, -2.5055e-01],\n        [-1.5340e+00, -1.3430e+00,  1.0605e+00, -4.6599e-01,  1.1229e+00,\n          7.7077e-01,  2.6036e+00, -6.0378e-02, -1.4935e-02,  2.2739e+00],\n        [-9.3102e-01,  1.6981e+00,  1.6502e-01, -1.3539e+00,  2.1162e+00,\n         -9.1690e-02, -1.6920e-01, -1.4731e-01,  1.8257e+00,  7.4139e-02],\n        [ 1.0196e+00, -1.5211e-01, -1.3747e-01, -1.9782e+00, -6.0507e-01,\n          3.8600e-01,  4.8492e-01, -7.1047e-01, -1.4910e-01, -7.0042e-01],\n        [ 7.9844e-01, -2.5242e-01,  7.6545e-01, -1.2806e+00, -2.6378e-01,\n         -1.5769e+00, -5.1581e-01,  3.9962e-01,  2.0376e+00,  7.0125e-01],\n        [-2.6719e-01,  7.5072e-02, -4.7886e-01, -9.0790e-01, -9.0978e-01,\n          2.3557e+00, -5.3002e-01,  3.0567e-01,  1.5483e-01, -2.4663e-01],\n        [-1.3229e+00,  6.5422e-01, -1.2638e-01,  2.4931e-01, -2.2307e-01,\n         -1.0068e+00, -7.3730e-01, -4.9129e-01,  5.8507e-01,  1.2266e+00],\n        [ 2.0285e-01,  1.4057e+00, -2.0167e+00,  8.5915e-01, -4.6512e-02,\n         -2.7763e-01,  1.4193e+00, -2.3945e-01,  1.0379e+00,  5.9219e-01],\n        [-9.2992e-01, -1.3255e+00, -4.6428e-01, -1.9135e+00, -3.7943e-01,\n          1.2328e-01, -1.0297e+00,  1.3015e+00, -1.7744e+00, -3.4169e-01],\n        [-5.5236e-01, -7.9126e-02,  1.1177e+00,  9.7907e-01,  1.0624e+00,\n          5.5756e-01, -1.1557e+00, -4.6458e-01, -5.0307e-02,  4.0598e-01],\n        [-8.8253e-01, -1.5947e+00,  1.2884e+00, -1.4321e+00,  1.4645e-01,\n         -4.4690e-01, -9.5914e-01, -2.1484e+00,  4.7363e-01, -1.8807e+00],\n        [ 1.8723e+00, -2.2935e-01,  2.3427e-01,  5.2890e-01, -4.2226e-02,\n          6.8002e-01,  9.8170e-01, -3.1499e-01, -1.3109e+00, -6.2617e-01]],\n       requires_grad=True)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight  # 训练好的词嵌入矩阵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "py38tg1",
   "language": "python",
   "display_name": "py38tg1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
